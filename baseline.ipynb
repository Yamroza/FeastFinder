{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adb0a65b80096e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:36:54.761784Z",
     "start_time": "2024-09-21T17:36:54.755678Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c95076341af215e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:36:55.614696Z",
     "start_time": "2024-09-21T17:36:55.583954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inform', 'im looking for a moderately priced restaurant that serves')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_set = []\n",
    "y_set = []\n",
    "\n",
    "with open(\"dialog_acts.dat\", 'r') as file:\n",
    "    for line in file:\n",
    "        y_set.append(line.split()[0])\n",
    "        x_set.append(\" \".join(line.split()[1:]).lower())\n",
    "\n",
    "y_set[0], x_set[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16255f5c4fb96381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:36:56.129738Z",
     "start_time": "2024-09-21T17:36:56.115970Z"
    }
   },
   "outputs": [],
   "source": [
    "#maybe split it in train, test and dev?\n",
    "\n",
    "x_train, x_test = x_set[int(len(x_set)*0.15):], x_set[:int(len(x_set)*0.15)]\n",
    "y_train, y_test = y_set[int(len(y_set)*0.15):], y_set[:int(len(y_set)*0.15)]\n",
    "len(x_train), len(x_test), len(y_train), len(y_test)\n",
    "\n",
    "x_train_no_dupl = set(x_train)\n",
    "x_test_no_dupl = set(x_test)\n",
    "print(len(x_train))\n",
    "print(len(x_test_no_dupl))\n",
    "\n",
    "y_train = pd.Series(y_train)\n",
    "y_test = pd.Series(y_test)\n",
    "y_train, word_list = pd.factorize(y_train)\n",
    "y_test = pd.factorize(y_test)[0]\n",
    "\n",
    "def to_cat(data, num_tokens):\n",
    "    encoder = keras.layers.CategoryEncoding(num_tokens=num_tokens, output_mode=\"one_hot\")\n",
    "    y_train_cat = encoder(data)\n",
    "    return y_train_cat\n",
    "\n",
    "y_train_cat= to_cat(y_train, 15)\n",
    "y_test_cat = to_cat(y_test, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288e8730187a965",
   "metadata": {},
   "source": [
    "## Dummy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be0ba654c1572a6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:36:57.361215Z",
     "start_time": "2024-09-21T17:36:57.358216Z"
    }
   },
   "outputs": [],
   "source": [
    "def dummy_baseline(query: str, word_list: list, y_train: np.array) -> str:\n",
    "    return word_list[np.bincount(y_train).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d55262a09cf4b853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:36:57.766608Z",
     "start_time": "2024-09-21T17:36:57.762529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inform'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_baseline('hi', word_list, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76637fd2003de0d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:36:58.413159Z",
     "start_time": "2024-09-21T17:36:58.082241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40261437908496733"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_basic = x_set[:int(len(x_set)*0.15)]\n",
    "y_test_basic = y_set[:int(len(y_set)*0.15)]\n",
    "\n",
    "suma = 0\n",
    "for x, y in zip(x_test_basic, y_test_basic):\n",
    "    if dummy_baseline(x, word_list, y_train) == y:\n",
    "        suma += 1\n",
    "\n",
    "dummy_acc = suma / len(x_test_basic)\n",
    "dummy_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cdda5d19f02aa",
   "metadata": {},
   "source": [
    "## Rule-based baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d555b8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:37:00.637388Z",
     "start_time": "2024-09-21T17:37:00.633823Z"
    }
   },
   "outputs": [],
   "source": [
    "def rule_based_baseline(sentence: str, keyword_dict: dict):\n",
    "    \"\"\"\n",
    "    Function returns a encoded category, encoding matches what a neural network would return.\n",
    "    This is done for ease of integration with other systems that may be built during this project.\n",
    "    :param sentence: sentence\n",
    "    :param keyword_dict: category-keyword\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    result = [0] * 15\n",
    "    count = [0] * 15\n",
    "    words = sentence.split()\n",
    "    i = 0\n",
    "    for keywords in keyword_dict.values():  # Iterating over categories\n",
    "        for key in keywords:                # Iterating over keywords in the categories\n",
    "            for word in words:              # Iterating over words in the sentence\n",
    "                if key == word:\n",
    "                    count[i] += 1\n",
    "        i += 1\n",
    "    max_keywards = max(count)                           # Finding number of the most maching keywords within a category\n",
    "    index_of_max_keywords =  count.index(max_keywards)  # Finding the index of the category with the most matching keywords\n",
    "    result[index_of_max_keywords] = 1                   # Encoding the category \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be8c85bc479a443e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:37:01.384504Z",
     "start_time": "2024-09-21T17:37:01.379456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Category-keyword dictionary\n",
    "\n",
    "keyword_dict = {\n",
    "    'ack': ['kay', 'okay', 'good', 'fine'],\n",
    "    'affirm': ['yes', 'right', 'correct', 'yeah', 'ye', 'right', 'correct', 'perfect'],\n",
    "    'bye': ['good', 'bye'],\n",
    "    'confirm': ['does', 'is', 'it'],\n",
    "    'deny': ['wrong', 'want', 'dont'],\n",
    "    'hello': ['hi', 'hello', 'im', 'looking'],\n",
    "    'inform': ['any', 'food', 'dont', 'care', 'expensive', 'moderate', 'cheap', 'east', 'west', 'north', 'south', 'centre', 'town', 'area', 'im', 'need', 'restaurant', 'looking'],\n",
    "    'negate': ['no'],\n",
    "    'null': ['unintelligible', 'noise', 'what', 'uh', 'sil', 'laughing'],\n",
    "    'repeat': ['repeat', 'back', 'go', 'again'],\n",
    "    'reqalts': ['else', 'next', 'how', 'about', 'any', 'anything', 'is', 'there', 'other'],\n",
    "    'reqmore': ['more'],\n",
    "    'request': ['type', 'phone', 'number', 'address', 'post', 'code', 'could', 'what', 'is', 'the', 'type', 'whats', 'may', 'i'],\n",
    "    'restart': ['start', 'over', 'reset'],\n",
    "    'thankyou': ['thank', 'you', 'good', 'bye', 'goodbye'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59ee0fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:37:01.936683Z",
     "start_time": "2024-09-21T17:37:01.931445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Category-code dictionary\n",
    "\n",
    "result_dict = {\n",
    "    'ack': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'affirm': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'bye': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'confirm': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'deny': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'hello': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'inform': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'negate': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'null': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    'repeat': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    'reqalts': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'reqmore': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'request': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    'restart': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    'thankyou': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6392ba8b69078498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T17:37:02.605447Z",
     "start_time": "2024-09-21T17:37:02.552613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate is:  0.8339869281045752\n"
     ]
    }
   ],
   "source": [
    "# Making predictions and calculating the success rate\n",
    "\n",
    "positive = 0\n",
    "for x in range(len(x_test)): # Iterating over sentences in the set\n",
    "    pred_coded = rule_based_baseline(x_test[x], keyword_dict) # Calling the predicting function to get the coded predicted category\n",
    "    for key, value in result_dict.items(): # Decoding categories\n",
    "        if pred_coded == value:\n",
    "            pred = key\n",
    "    if pred == y_test_basic[x]: # Adding 1 to successful prediction count if the prediction matches data\n",
    "        positive += 1\n",
    "rate = positive / len(y_test_basic) # Calculating and displaying a success rate\n",
    "print('Rate is: ', rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91571833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec (Sentences to vector)\n",
    "# We use this to convert the whole input paragraph to a vector\n",
    "# Upon reading the exercise again, they seem to suggest using bag-of-words. However, I think this is better. Lets discuss with the TA!\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "tagged_x_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(x_train)]\n",
    "model = Doc2Vec(vector_size=50,\n",
    "                min_count=2, epochs=50)\n",
    "#print(tagged_x_data)\n",
    "model.build_vocab(tagged_x_data)\n",
    "model.train(tagged_x_data,\n",
    "            total_examples=model.corpus_count,\n",
    "            epochs=model.epochs)\n",
    "\n",
    "def to_vector(list_of_words):\n",
    "    vectorized = [model.infer_vector(word_tokenize(doc.lower())) for doc in list_of_words]\n",
    "    return np.array(vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import regex as re\n",
    "def tokenization(data):\n",
    "    tokenized = []\n",
    "    for sent in data:\n",
    "        tokens = word_tokenize(sent)\n",
    "        token_sent = [w.lower() for w in tokens if w.isalpha() ]\n",
    "        tokenized.extend(token_sent)\n",
    "    tokenized = sorted(list(set(tokenized)))\n",
    "    return tokenized\n",
    "\n",
    "def word_extraction(sentence):    \n",
    "    #ignore = ['a', \"the\", \"is\"]\n",
    "    ignore = []    \n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def generate_vec(data, vocab):\n",
    "    vectors =[]\n",
    "    for sentence in data:                \n",
    "        bag_vector = np.zeros(len(vocab))  \n",
    "        words = word_extraction(sentence)    \n",
    "        for w in words:            \n",
    "            for i,word in enumerate(vocab):               \n",
    "                if word == w:                     \n",
    "                    bag_vector[i] += 1\n",
    "        vectors.append(bag_vector)\n",
    "    return vectors\n",
    "\n",
    "def generate_bow(train, test):       \n",
    "    vocab = tokenization(train)  \n",
    "    train =generate_vec(train, vocab)\n",
    "    test = generate_vec(test, vocab)\n",
    "    return train, test\n",
    "\n",
    "x_train_bow, x_test_bow = generate_bow(x_train, x_test)\n",
    "#bag_of_words(x_train_tokenized, vocab, word_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d18a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = to_vector(x_train)\n",
    "x_test_vector = to_vector(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b8fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, epochs):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history[\"loss\"], label = \"loss\")\n",
    "    ax.plot(history.history[\"val_loss\"], label = \"Valditation loss\")\n",
    "    ax.set_title(f\"Loss in {epochs} epochs\")\n",
    "    fig.legend()\n",
    "\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    ax2.plot(history.history[\"accuracy\"], label = \"accuracy\")\n",
    "    ax2.plot(history.history[\"val_accuracy\"], label = \"Validation accuracy\")\n",
    "    ax2.set_title(f\"Accuracy in {epochs} epochs\")\n",
    "    fig2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FNN with BOW\n",
    "\n",
    "epochs = 12\n",
    "model_FNN = keras.Sequential()\n",
    "model_FNN.add(keras.Input(shape=(731,)))\n",
    "model_FNN.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "model_FNN.add(keras.layers.Dense(15, activation=\"softmax\"))\n",
    "\n",
    "model_FNN.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_FNN.fit(np.array(x_train_bow), y_train_cat, batch_size=64,\n",
    "epochs=epochs, verbose=1,validation_data=(np.array(x_test_bow), y_test_cat))\n",
    "\n",
    "plot(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FNN with word embeddings\n",
    "epochs = 12\n",
    "model_FNN = keras.Sequential()\n",
    "model_FNN.add(keras.Input(shape=(50,)))\n",
    "model_FNN.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "model_FNN.add(keras.layers.Dense(15, activation=\"softmax\"))\n",
    "\n",
    "model_FNN.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_FNN.fit(document_vectors, y_train_cat, batch_size=64,\n",
    "epochs=12, verbose=1,validation_data=(x_test_vector, y_test_cat))\n",
    "\n",
    "plot(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f248e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with BOW\n",
    "model_LR = keras.Sequential()\n",
    "model_LR.add(keras.Input(shape=(731,)))\n",
    "model_LR.add(keras.layers.Dense(15, activation=\"softmax\"))\n",
    "\n",
    "model_LR.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_LR.fit(np.array(x_train_bow), y_train_cat, batch_size=64, epochs=12, verbose=1,\n",
    "                       validation_data=(np.array(x_test_bow), y_test_cat))\n",
    "\n",
    "plot(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26781d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with word embeddings\n",
    "model_LR = keras.Sequential()\n",
    "model_LR.add(keras.Input(shape=(50,)))\n",
    "model_LR.add(keras.layers.Dense(15, activation=\"softmax\"))\n",
    "\n",
    "model_LR.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_LR.fit(document_vectors, y_train_cat, batch_size=64, epochs=12, verbose=1,\n",
    "                       validation_data=(x_test_vector, y_test_cat))\n",
    "\n",
    "plot(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = [\"yes\", \"thai food\", \"what is the cheapest restaurant in london?\"]\n",
    "# make a prediction\n",
    "def prediction(Xnew):\n",
    "    X_new_vector = to_vector(Xnew)\n",
    "    X_new_vector = np.array(X_new_vector)\n",
    "    ynew = model_LR.predict(X_new_vector)\n",
    "# show the inputs and predicted outputs\n",
    "\n",
    "    for i in range(len(Xnew)):\n",
    "        print(\"X=%s, Predicted=%s\" % (Xnew[i], word_list[np.argmax(ynew[i])]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
