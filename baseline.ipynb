{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb0a65b80096e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:29:45.565824Z",
     "start_time": "2024-09-12T08:29:45.562421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\karst\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('inform', 'im looking for a moderately priced restaurant that serves')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x_set = []\n",
    "y_set = []\n",
    "\n",
    "with open(\"dialog_acts.dat\", 'r') as file:\n",
    "    for line in file:\n",
    "        y_set.append(line.split()[0])\n",
    "        x_set.append(\" \".join(line.split()[1:]).lower())\n",
    "    # dialog_dataset = file.read()\n",
    "\n",
    "y_set[0], x_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a178cdbe772690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:29:46.642610Z",
     "start_time": "2024-09-12T08:29:46.640047Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_train, x_test = x_set[int(len(x_set)*0.15):], x_set[:int(len(x_set)*0.15)]\n",
    "# y_train, y_test = y_set[int(len(y_set)*0.15):], y_set[:int(len(y_set)*0.15)]\n",
    "# len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16255f5c4fb96381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:29:47.273011Z",
     "start_time": "2024-09-12T08:29:47.259152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21676\n",
      "1297\n"
     ]
    }
   ],
   "source": [
    "#maybe split it in train, test and dev?\n",
    "\n",
    "x_train, x_test = x_set[int(len(x_set)*0.15):], x_set[:int(len(x_set)*0.15)]\n",
    "y_train, y_test = y_set[int(len(y_set)*0.15):], y_set[:int(len(y_set)*0.15)]\n",
    "len(x_train), len(x_test), len(y_train), len(y_test)\n",
    "\n",
    "x_train_no_dupl = set(x_train)\n",
    "x_test_no_dupl = set(x_test)\n",
    "print(len(x_train))\n",
    "print(len(x_test_no_dupl))\n",
    "y_train = pd.Series(y_train)\n",
    "y_test = pd.Series(y_test)\n",
    "y_train, word_list = pd.factorize(y_train)\n",
    "y_test = pd.factorize(y_test)[0]\n",
    "def to_cat(data, num_tokens):\n",
    "    encoder = keras.layers.CategoryEncoding(num_tokens=num_tokens, output_mode=\"one_hot\")\n",
    "    y_train_cat = encoder(data)\n",
    "    return y_train_cat\n",
    "\n",
    "\n",
    "y_train_cat= to_cat(y_train, 15)\n",
    "y_test_cat = to_cat(y_test, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288e8730187a965",
   "metadata": {},
   "source": [
    "## Dummy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be0ba654c1572a6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:29:57.086022Z",
     "start_time": "2024-09-12T08:29:57.083224Z"
    }
   },
   "outputs": [],
   "source": [
    "def dummy_baseline(query: str) -> str:\n",
    "    return max(set(y_train), key=y_train.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d55262a09cf4b853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:29:57.850869Z",
     "start_time": "2024-09-12T08:29:57.839787Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdummy_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mdummy_baseline\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdummy_baseline\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mset\u001b[39m(y_train), key\u001b[38;5;241m=\u001b[39m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "print(dummy_baseline('hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76637fd2003de0d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T08:33:07.519561Z",
     "start_time": "2024-09-12T08:33:07.516533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40041819132253004\n"
     ]
    }
   ],
   "source": [
    "dummy_baseline_acc = y_test.count('inform') / len(y_test)\n",
    "print(dummy_baseline_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2cdda5d19f02aa",
   "metadata": {},
   "source": [
    "## Rule-based baseline : TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d555b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes as arguments a sentence and a category-keyword dictionary.\n",
    "\n",
    "def rule_based_baseline(sentence, keyword_dict):\n",
    "    result = [0] * 15\n",
    "    count = [0] * 15\n",
    "    words = sentence.split()\n",
    "    i = 0\n",
    "    for keywords in keyword_dict.values(): # Iterating over categories\n",
    "        for key in keywords: # Iterating over keywords in the categories\n",
    "            for word in words: # Iterating over words in the sentence\n",
    "                if key == word:\n",
    "                    count[i] += 1\n",
    "        i += 1\n",
    "    max_keywards = max(count) # Finding number of the most maching keywords within a category\n",
    "    index_of_max_keywords =  count.index(max_keywards) # Finding the index of the category with the most matching keywords\n",
    "    result[index_of_max_keywords] = 1 # Encoding the categry \n",
    "    return result # Function returns a encoded category, encoding matches what a neural network would return.\n",
    "    # This is done for ease of inegration with other systems that may be build during this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "be8c85bc479a443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-keyword dictionary\n",
    "\n",
    "keyword_dict = {\n",
    "    'ack': ['kay', 'okay', 'good', 'fine'],\n",
    "    'affirm': ['yes', 'right', 'correct', 'yeah', 'ye', 'right', 'correct', 'perfect'],\n",
    "    'bye': ['good', 'bye'],\n",
    "    'confirm': ['does', 'is', 'it'],\n",
    "    'deny': ['wrong', 'want', 'dont'],\n",
    "    'hello': ['hi', 'hello', 'im', 'looking'],\n",
    "    'inform': ['any', 'food', 'dont', 'care', 'expensive', 'moderate', 'cheap', 'east', 'west', 'north', 'south', 'centre', 'town', 'area', 'im', 'need', 'restaurant', 'looking'],\n",
    "    'negate': ['no'],\n",
    "    'null': ['unintelligible', 'noise', 'what', 'uh', 'sil', 'laughing'],\n",
    "    'repeat': ['repeat', 'back', 'go', 'again'],\n",
    "    'reqalts': ['else', 'next', 'how', 'about', 'any', 'anything', 'is', 'there', 'other'],\n",
    "    'reqmore': ['more'],\n",
    "    'request': ['type', 'phone', 'number', 'address', 'post', 'code', 'could', 'what', 'is', 'the', 'type', 'whats', 'may', 'i'],\n",
    "    'restart': ['start', 'over', 'reset'],\n",
    "    'thankyou': ['thank', 'you', 'good', 'bye', 'goodbye'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "59ee0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-code dictionary\n",
    "\n",
    "result_dict = {\n",
    "    'ack': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'affirm': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'bye': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'confirm': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'deny': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'hello': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'inform': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'negate': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'null': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    'repeat': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    'reqalts': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'reqmore': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'request': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    'restart': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    'thankyou': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6392ba8b69078498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate:  0.8429168844746472\n"
     ]
    }
   ],
   "source": [
    "# Making predictions and calculating the success reate\n",
    "\n",
    "positive = 0\n",
    "for x in range(len(x_test)): # Iterating over sentences in the set\n",
    "    pred_coded = rule_based_baseline(x_test[x], keyword_dict) # Callin the predicting function to get the coded predicted category\n",
    "    for key, value in result_dict.items(): # Decoding categories\n",
    "        if pred_coded == value:\n",
    "            pred = key\n",
    "    if pred == y_test[x]: # Adding 1 to successful predition count if the prediction matches data\n",
    "        positive += 1\n",
    "rate = positive / len(y_test) # Calcualting and displaying a success rate\n",
    "print('Rate is: ', rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91571833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\karst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Doc2Vec (Sentences to vector)\n",
    "#We use this to convert the whole input paragraph to a vector\n",
    "#Upon reading the exercise again, they seem to suggest using bag-of-words. However, I think this is better. Lets discuss with the TA!\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "tagged_x_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(x_train)]\n",
    "model = Doc2Vec(vector_size=50,\n",
    "                min_count=2, epochs=50)\n",
    "#print(tagged_x_data)\n",
    "model.build_vocab(tagged_x_data)\n",
    "model.train(tagged_x_data,\n",
    "            total_examples=model.corpus_count,\n",
    "            epochs=model.epochs)\n",
    "\n",
    "def to_vector(list_of_words):\n",
    "    vectorized = [model.infer_vector(word_tokenize(doc.lower())) for doc in list_of_words]\n",
    "    return np.array(vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import regex as re\n",
    "def tokenization(data):\n",
    "    tokenized = []\n",
    "    for sent in data:\n",
    "        tokens = word_tokenize(sent)\n",
    "        token_sent = [w.lower() for w in tokens if w.isalpha() ]\n",
    "        tokenized.extend(token_sent)\n",
    "    tokenized = sorted(list(set(tokenized)))\n",
    "    return tokenized\n",
    "\n",
    "def word_extraction(sentence):    \n",
    "    #ignore = ['a', \"the\", \"is\"]\n",
    "    ignore = []    \n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def generate_vec(data, vocab):\n",
    "    vectors =[]\n",
    "    for sentence in data:                \n",
    "        bag_vector = np.zeros(len(vocab))  \n",
    "        words = word_extraction(sentence)    \n",
    "        for w in words:            \n",
    "            for i,word in enumerate(vocab):               \n",
    "                if word == w:                     \n",
    "                    bag_vector[i] += 1\n",
    "        vectors.append(bag_vector)\n",
    "    return vectors\n",
    "\n",
    "def generate_bow(train, test):       \n",
    "    vocab = tokenization(train)  \n",
    "    train =generate_vec(train, vocab)\n",
    "    test = generate_vec(test, vocab)\n",
    "    return train, test\n",
    "\n",
    "x_train_bow, x_test_bow = generate_bow(x_train, x_test)\n",
    "#bag_of_words(x_train_tokenized, vocab, word_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d18a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = to_vector(x_train)\n",
    "x_test_vector = to_vector(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b8fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, epochs):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history[\"loss\"], label = \"loss\")\n",
    "    ax.plot(history.history[\"val_loss\"], label = \"Valditation loss\")\n",
    "    ax.set_title(f\"Loss in {epochs} epochs\")\n",
    "    fig.legend()\n",
    "\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    ax2.plot(history.history[\"accuracy\"], label = \"accuracy\")\n",
    "    ax2.plot(history.history[\"val_accuracy\"], label = \"Validation accuracy\")\n",
    "    ax2.set_title(f\"Accuracy in {epochs} epochs\")\n",
    "    fig2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FNN with BOW\n",
    "\n",
    "epochs = 12\n",
    "model_FNN = keras.Sequential()\n",
    "model_FNN.add(keras.Input(shape=(731,)))\n",
    "model_FNN.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "model_FNN.add(keras.layers.Dense(15, activation=\"softmax\"))\n",
    "\n",
    "model_FNN.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_FNN.fit(np.array(x_train_bow), y_train_cat, batch_size=64,\n",
    "epochs=epochs, verbose=1,validation_data=(np.array(x_test_bow), y_test_cat))\n",
    "\n",
    "plot(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FNN with word embeddings\n",
    "epochs = 12\n",
    "model_FNN = keras.Sequential()\n",
    "model_FNN.add(keras.Input(shape=(50,)))\n",
    "model_FNN.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "model_FNN.add(keras.layers.Dense(15, activation=\"softmax\"))\n",
    "\n",
    "model_FNN.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_FNN.fit(document_vectors, y_train_cat, batch_size=64,\n",
    "epochs=12, verbose=1,validation_data=(x_test_vector, y_test_cat))\n",
    "\n",
    "plot(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f248e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with BOW\n",
    "model_LR = keras.Sequential()\n",
    "model_LR.add(keras.Input(shape=(731,)))\n",
    "model_LR.add(keras.layers.Dense(15, activation=\"softmax\"))\n",
    "\n",
    "model_LR.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_LR.fit(np.array(x_train_bow), y_train_cat, batch_size=64, epochs=12, verbose=1,\n",
    "                       validation_data=(np.array(x_test_bow), y_test_cat))\n",
    "\n",
    "plot(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26781d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with word embeddings\n",
    "model_LR = keras.Sequential()\n",
    "model_LR.add(keras.Input(shape=(50,)))\n",
    "model_LR.add(keras.layers.Dense(15, activation=\"softmax\"))\n",
    "\n",
    "model_LR.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model_LR.fit(document_vectors, y_train_cat, batch_size=64, epochs=12, verbose=1,\n",
    "                       validation_data=(x_test_vector, y_test_cat))\n",
    "\n",
    "plot(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = [\"yes\", \"thai food\", \"what is the cheapest restaurant in london?\"]\n",
    "# make a prediction\n",
    "X_new_vector = to_vector(Xnew)\n",
    "X_new_vector = np.array(X_new_vector)\n",
    "ynew = model_LR.predict(X_new_vector)\n",
    "# show the inputs and predicted outputs\n",
    "\n",
    "for i in range(len(Xnew)):\n",
    " print(\"X=%s, Predicted=%s\" % (Xnew[i], word_list[np.argmax(ynew[i])]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
